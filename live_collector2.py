# collector.py (SET ... NX ë²„ì „)
# ì„¤ê³„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€:
#   âœ… SADD proxies:pool proxy
#   âœ… SET proxy "<meta>" EX 21600  (proxy ë¬¸ìì—´ì„ keyë¡œ TTL)
#
# ê°œì„ ì (ì¤‘ìš”):
#   - SETì— NXë¥¼ ë¶™ì—¬ì„œ "í‚¤ê°€ ì—†ì„ ë•Œë§Œ" ìƒì„± + TTL ì„¤ì •
#   - ì´ë¯¸ ì¡´ì¬í•˜ëŠ” proxy keyëŠ” TTLì„ ê°±ì‹ í•˜ì§€ ì•ŠìŒ
#     => ì¬ì‹¤í–‰/ë°˜ë³µ ì‹¤í–‰í•´ë„ ë§¤ë²ˆ 19ë§Œê±´ TTL ê°±ì‹ ìœ¼ë¡œ ëŠë ¤ì§€ëŠ” ë¬¸ì œ í•´ê²°
#     => TTL 6ì‹œê°„ì˜ ì˜ë¯¸(ì˜¤ë˜ëœ ê±´ ìì—°íˆ ì‚¬ë¼ì§)ë„ ë” ì„ ëª…í•´ì§
#
# ì„¤ì¹˜:
#   pip install requests redis
# ì‹¤í–‰:
#   python collector.py

import json
import random
import signal
import time
from datetime import datetime, timezone
from typing import Dict, List, Optional, Set, Tuple

import redis
import requests

# =========================
# í”„ë¡ì‹œ ì†ŒìŠ¤ URL
# =========================
MONOSANS_HTTP = "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt"
MONOSANS_SOCKS4 = "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks4.txt"
MONOSANS_SOCKS5 = "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt"

VICTORGEEL_HTTP = "https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/http.txt"
VICTORGEEL_SOCKS4 = "https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/socks4.txt"
VICTORGEEL_SOCKS5 = "https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/socks5.txt"

ERCINDEDEOGLU_HTTP = "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/http.txt"
ERCINDEDEOGLU_HTTPS = "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/https.txt"
ERCINDEDEOGLU_SOCKS4 = "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/socks4.txt"
ERCINDEDEOGLU_SOCKS5 = "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/socks5.txt"

VAKHOV_HTTP = "https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/http.txt"
VAKHOV_HTTPS = "https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/https.txt"
VAKHOV_SOCKS4 = "https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/socks4.txt"
VAKHOV_SOCKS5 = "https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/socks5.txt"

SOURCES: List[Tuple[str, str, str]] = [
    (MONOSANS_HTTP, "http", "monosans_http"),
    (MONOSANS_SOCKS4, "socks4", "monosans_socks4"),
    (MONOSANS_SOCKS5, "socks5", "monosans_socks5"),
    (VICTORGEEL_HTTP, "http", "victorgeel_http"),
    (VICTORGEEL_SOCKS4, "socks4", "victorgeel_socks4"),
    (VICTORGEEL_SOCKS5, "socks5", "victorgeel_socks5"),
    (ERCINDEDEOGLU_HTTP, "http", "ercindedeoglu_http"),
    (ERCINDEDEOGLU_HTTPS, "https", "ercindedeoglu_https"),
    (ERCINDEDEOGLU_SOCKS4, "socks4", "ercindedeoglu_socks4"),
    (ERCINDEDEOGLU_SOCKS5, "socks5", "ercindedeoglu_socks5"),
    (VAKHOV_HTTP, "http", "vakhov_http"),
    (VAKHOV_HTTPS, "https", "vakhov_https"),
    (VAKHOV_SOCKS4, "socks4", "vakhov_socks4"),
    (VAKHOV_SOCKS5, "socks5", "vakhov_socks5"),
]

# =========================
# ì„¤ì •
# =========================
REDIS_HOST = "127.0.0.1"
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_PASSWORD = None

POOL_KEY = "proxies:pool"

TTL_SECONDS = 21600  # 6h
COLLECT_INTERVAL_MINUTES = 30

FETCH_TIMEOUT = 30
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"

# ëŒ€ëŸ‰ ì €ì¥ ë©ˆì¶¤ ë°©ì§€ìš© ì²­í¬
REDIS_CHUNK_SIZE = 5000

# ë„ˆë¬´ ë§ìœ¼ë©´ ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ìƒ˜í”Œë§(ì›í•˜ë©´ None)
MAX_ADD_PER_ROUND: Optional[int] = None  # ì˜ˆ: 50000 / None

STOP = False


def _handle_sigint(sig, frame):
    global STOP
    STOP = True
    print("\nğŸ›‘ Ctrl+C ê°ì§€: ê°€ëŠ¥í•œ ë¹¨ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤(í˜„ì¬ ì‘ì—…/ì²­í¬ ì™„ë£Œ í›„ ì¢…ë£Œ).")


signal.signal(signal.SIGINT, _handle_sigint)


def utc_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def get_redis() -> redis.Redis:
    return redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        db=REDIS_DB,
        password=REDIS_PASSWORD,
        decode_responses=True,
        socket_connect_timeout=10,
        socket_timeout=30,
        retry_on_timeout=True,
    )


def normalize_line_to_addr(line: str) -> Optional[str]:
    s = line.strip()
    if not s or s.startswith("#"):
        return None

    if "://" in s:
        s = s.split("://", 1)[1]

    s = s.split("/", 1)[0].strip()
    if ":" not in s:
        return None

    host, port = s.rsplit(":", 1)
    host = host.strip()
    port = port.strip()

    if not host or not port.isdigit():
        return None

    p = int(port)
    if p <= 0 or p > 65535:
        return None

    return f"{host}:{p}"


def fetch_source(url: str, protocol: str, source_name: str) -> List[str]:
    if STOP:
        return []

    print(f"ğŸ“¥ GET {source_name:20s} ({protocol})")
    try:
        resp = requests.get(url, timeout=FETCH_TIMEOUT, headers={"User-Agent": UA})
        resp.raise_for_status()

        out: List[str] = []
        for line in resp.text.splitlines():
            addr = normalize_line_to_addr(line)
            if not addr:
                continue
            out.append(f"{protocol}://{addr}")

        print(f"   âœ… parsed={len(out)}")
        return out
    except Exception as e:
        print(f"   âŒ fail: {type(e).__name__}: {str(e)[:140]}")
        return []


def collect_all_unique() -> Tuple[List[str], Dict[str, int]]:
    unique: Set[str] = set()
    stats = {"http": 0, "https": 0, "socks4": 0, "socks5": 0, "sources_ok": 0, "sources_total": len(SOURCES)}

    for url, proto, name in SOURCES:
        if STOP:
            break
        items = fetch_source(url, proto, name)
        if items:
            stats["sources_ok"] += 1
        unique.update(items)
        time.sleep(0.15)

    for m in unique:
        if m.startswith("http://"):
            stats["http"] += 1
        elif m.startswith("https://"):
            stats["https"] += 1
        elif m.startswith("socks4://"):
            stats["socks4"] += 1
        elif m.startswith("socks5://"):
            stats["socks5"] += 1

    return list(unique), stats


def iter_chunks(items: List[str], chunk_size: int):
    for i in range(0, len(items), chunk_size):
        yield items[i : i + chunk_size]


def redis_save_chunked_nx(r: redis.Redis, proxies: List[str]) -> Tuple[int, int]:
    """
    ì„¤ê³„ ê³ ì • + NX ìµœì í™”:
      - SADD proxies:pool proxy
      - SET proxy "<meta>" EX 21600 NX  (ì—†ì„ ë•Œë§Œ ìƒì„±)
    ë°˜í™˜:
      (pool_added_total, keys_created_total)
    """
    if not proxies:
        return (0, 0)

    if MAX_ADD_PER_ROUND is not None and len(proxies) > MAX_ADD_PER_ROUND:
        proxies = random.sample(proxies, MAX_ADD_PER_ROUND)

    total = len(proxies)
    ts = utc_iso()

    pool_added_total = 0
    keys_created_total = 0

    print(f"ğŸ’¾ Redis ì €ì¥(ì²­í¬+NX): total={total}, chunk={REDIS_CHUNK_SIZE}")

    # 1) pool ì €ì¥ (SADD) - ì²­í¬
    done = 0
    for ck in iter_chunks(proxies, REDIS_CHUNK_SIZE):
        if STOP:
            break
        done += len(ck)
        try:
            added = r.sadd(POOL_KEY, *ck)
            pool_added_total += int(added) if added is not None else 0
            print(f"  [POOL] {done}/{total} | +{added} new")
        except KeyboardInterrupt:
            raise
        except Exception as e:
            print(f"  âš ï¸ [POOL] chunk fail: {type(e).__name__}: {str(e)[:160]}")

    if STOP:
        return (pool_added_total, keys_created_total)

    # 2) key ì €ì¥ (SET EX NX) - ì²­í¬
    done = 0
    for ck in iter_chunks(proxies, REDIS_CHUNK_SIZE):
        if STOP:
            break

        pipe = r.pipeline(transaction=False)

        # SET ... NX ëŠ” ì„±ê³µ ì‹œ True/OK, ì‹¤íŒ¨(ì´ë¯¸ ì¡´ì¬) ì‹œ None
        for p in ck:
            meta = {"collected_at": ts}
            pipe.set(p, json.dumps(meta, ensure_ascii=False), ex=TTL_SECONDS, nx=True)

        try:
            results = pipe.execute()
            created = sum(1 for x in results if x)  # True/OK count
            keys_created_total += created
            done += len(ck)
            print(f"  [KEYS] {done}/{total} | created={created} (NX)")
        except KeyboardInterrupt:
            raise
        except Exception as e:
            print(f"  âš ï¸ [KEYS] chunk fail: {type(e).__name__}: {str(e)[:160]}")

    return (pool_added_total, keys_created_total)


def main_loop():
    r = get_redis()
    try:
        r.ping()
        print("âœ… Redis PING OK")
    except Exception as e:
        print(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return

    print("=" * 80)
    print("ğŸš€ collector (fixed design + NX optimization)")
    print("âœ… SADD proxies:pool proxy")
    print("âœ… SET proxy '<meta>' EX 21600 NX  (í‚¤ ì—†ì„ ë•Œë§Œ ìƒì„±)")
    print(f"â€¢ interval: {COLLECT_INTERVAL_MINUTES} min | chunk: {REDIS_CHUNK_SIZE}")
    print("ğŸ›‘ Ctrl+C ë¡œ ì¢…ë£Œ")
    print("=" * 80)

    while not STOP:
        t0 = time.time()
        print("\n" + "=" * 80)
        print(f"ğŸ• collect start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

        proxies, stats = collect_all_unique()
        print("-" * 80)
        print(
            f"ğŸ“¦ unique={len(proxies)} | "
            f"http={stats['http']} https={stats['https']} socks4={stats['socks4']} socks5={stats['socks5']} | "
            f"sources_ok={stats['sources_ok']}/{stats['sources_total']}"
        )
        print("-" * 80)

        try:
            pool_added, keys_created = redis_save_chunked_nx(r, proxies)
            pool_size = r.scard(POOL_KEY)
            print(f"âœ… redis done: pool_added={pool_added} keys_created={keys_created} pool_size={pool_size}")
        except KeyboardInterrupt:
            print("\nğŸ›‘ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        elapsed = time.time() - t0
        print(f"â±ï¸  elapsed: {elapsed:.1f}s")

        if STOP:
            break

        sleep_sec = max(5, COLLECT_INTERVAL_MINUTES * 60 - int(elapsed))
        print(f"ğŸ’¤ sleep {sleep_sec}s ...")
        for _ in range(sleep_sec):
            if STOP:
                break
            time.sleep(1)

    print("ğŸ‘‹ collector stopped.")


if __name__ == "__main__":
    main_loop()
