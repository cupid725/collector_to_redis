import os
import re
import json
import time
import csv
import socket
import shutil
import random
import logging
import tempfile
import threading
import struct
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from urllib3.connection import HTTPConnection
from pathlib import Path

import requests
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import (
    TimeoutException,
    WebDriverException,
)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =============================================================================
# 0) ì‚¬ìš©ì ì„¤ì •
# =============================================================================
MAX_THREADS = 1  

ENABLE_WINDOW_SIZE = True
WINDOW_WIDTH = 600
WINDOW_HEIGHT = 400
ENABLE_WINDOW_JITTER = False
WINDOW_JITTER_RANGE = 80
ENABLE_WINDOW_POSITION = True
WINDOW_POS_X = 50
WINDOW_POS_Y = 400
ENABLE_BLOCK_CHECK = False 
CHECK_INTERVAL_SECONDS = 60*30
MAX_PAGES = 10

TASKS = [
    {"keyword": "ì˜¬ë¹¼ë¯¸í‹°ë¹„", "domain": "https://www.tvda.co.kr/?srt=1"},
]

MAX_PROXIES_PER_TASK = 30
REFRESH_PROXIES_EACH_CYCLE = True
RUN_HEADLESS = False
PAGELOAD_TIMEOUT_SEC = 60*2
ELEM_WAIT_SEC = 30

# ğŸ”’ ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì„¤ì • (ì‹ ê·œ ì¶”ê°€)
ENABLE_STEALTH = True  # ìŠ¤í…”ìŠ¤ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
RANDOM_DELAY_MIN = 2.0  # ì•¡ì…˜ ê°„ ìµœì†Œ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
RANDOM_DELAY_MAX = 5.0  # ì•¡ì…˜ ê°„ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
ENABLE_MOUSE_MOVEMENT = True  # ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜
SCROLL_BEHAVIOR = True  # ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜

OUT_DIR = os.path.abspath("./naver_monitor_out")
LOG_FILE = os.path.join(OUT_DIR, "monitor.log")
RESULT_JSONL = os.path.join(OUT_DIR, "results.jsonl")
RESULT_CSV = os.path.join(OUT_DIR, "results.csv")
WINDOW_STATE_FILE = os.path.join(OUT_DIR, "window_states.json")  # ì°½ ìƒíƒœ ì €ì¥ íŒŒì¼
STOP_EVENT = threading.Event()
FILE_LOCK = threading.Lock()
WINDOW_STATE_LOCK = threading.Lock()  # ì°½ ìƒíƒœ íŒŒì¼ ì ‘ê·¼ìš© ë½ 

# ì „ì—­ ë³€ìˆ˜: ë‚´ ê³µì¸ IP ì €ì¥ìš©
MY_PUBLIC_IP = None

# =============================================================================
# 1) í”„ë¡ì‹œ ì„¤ì • (ê¸°ë³¸ ìœ ì§€)
# =============================================================================
ALL_SOURCES = [
    ("https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt", "http", False),
    ("https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks4.txt", "socks4", False),
    ("https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt", "socks5", False),
    ("https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/http.txt", "http", False),
    ("https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/socks4.txt", "socks4", False),
    ("https://raw.githubusercontent.com/victorgeel/proxy-list-update/main/proxies/socks5.txt", "socks5", False),
    ("https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/http.txt", "http", False),
    ("https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/socks4.txt", "socks4", False),
    ("https://raw.githubusercontent.com/vakhov/fresh-proxy-list/master/socks5.txt", "socks5", False),
]

SOURCES_KR = [
    ("https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc&country=KR&anonymityLevel=elite", "http", True),
]

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

linger_option = (socket.SOL_SOCKET, socket.SO_LINGER, struct.pack('ii', 1, 0))
HTTPConnection.default_socket_options = HTTPConnection.default_socket_options + [linger_option]

# =============================================================================
# 2) ë°ì´í„° ëª¨ë¸ ë° ìœ í‹¸ (ë¡œê·¸ ì„¤ì • ê°•í™”)
# =============================================================================
@dataclass
class ProxyInfo:
    protocol: str
    address: str
    source: str

@dataclass
class RunResult:
    ts: str
    keyword: str
    target_url: str
    proxy_protocol: Optional[str]
    proxy_address: Optional[str]
    proxy_source: Optional[str]
    found: bool
    found_page: Optional[int]
    found_rank_on_page: Optional[int]
    found_href: Optional[str]
    clicked_ok: bool
    final_url: Optional[str]
    error: Optional[str]
    note: Optional[str]

def setup_logging() -> None:
    os.makedirs(OUT_DIR, exist_ok=True)
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s", datefmt="%H:%M:%S")
    
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    fh.setFormatter(fmt)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    
    logger.handlers.clear()
    logger.addHandler(fh)
    logger.addHandler(sh)

# =============================================================================
# 2-1) ğŸ”’ ì¶”ê°€ëœ IP í™•ì¸ ë° ìŠ¤í…”ìŠ¤ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
def get_my_actual_ip():
    """ì‹¤í–‰ ì‹œì ì˜ ë‚´ ì‹¤ì œ ê³µì¸ IP í™•ì¸"""
    try:
        res = requests.get("https://api.ipify.org", timeout=10)
        return res.text.strip()
    except:
        return None

def is_proxy_leaking_my_ip(proxy: ProxyInfo, my_ip: str):
    """í”„ë¡ì‹œê°€ ë‚´ IPë¥¼ ìœ ì¶œí•˜ê³  ìˆëŠ”ì§€(ë‚´ IPê°€ ë°˜í™˜ë˜ëŠ”ì§€) í™•ì¸"""
    if not my_ip: return False # ë‚´ IPë¥¼ ëª¨ë¥´ë©´ ì²´í¬ ë¶ˆê°€í•˜ë¯€ë¡œ íŒ¨ìŠ¤
    try:
        proxies = {
            "http": f"{proxy.protocol}://{proxy.address}",
            "https": f"{proxy.protocol}://{proxy.address}"
        }
        res = requests.get("https://api.ipify.org", proxies=proxies, timeout=10)
        return res.text.strip() == my_ip
    except:
        return False

def load_window_state(slot_id: str) -> Optional[Dict]:
    try:
        with WINDOW_STATE_LOCK:
            if os.path.exists(WINDOW_STATE_FILE):
                with open(WINDOW_STATE_FILE, 'r', encoding='utf-8') as f:
                    states = json.load(f)
                    return states.get(slot_id)
    except Exception as e:
        logging.warning(f"âš ï¸ ì°½ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨ (ìŠ¬ë¡¯ {slot_id}): {e}")
    return None

def save_window_state(slot_id: str, x: int, y: int, width: int, height: int) -> None:
    try:
        with WINDOW_STATE_LOCK:
            states = {}
            if os.path.exists(WINDOW_STATE_FILE):
                with open(WINDOW_STATE_FILE, 'r', encoding='utf-8') as f:
                    states = json.load(f)
            states[slot_id] = {'x': x, 'y': y, 'width': width, 'height': height}
            with open(WINDOW_STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(states, f, indent=2)
            logging.info(f"ğŸ’¾ ì°½ ìƒíƒœ ì €ì¥ ì™„ë£Œ (ìŠ¬ë¡¯ {slot_id}): ìœ„ì¹˜({x},{y}) í¬ê¸°({width}x{height})")
    except Exception as e:
        logging.warning(f"âš ï¸ ì°½ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨ (ìŠ¬ë¡¯ {slot_id}): {e}")

def random_delay(min_sec: float = None, max_sec: float = None) -> None:
    if not ENABLE_STEALTH: return
    min_val = min_sec if min_sec is not None else RANDOM_DELAY_MIN
    max_val = max_sec if max_sec is not None else RANDOM_DELAY_MAX
    time.sleep(random.uniform(min_val, max_val))

def simulate_human_typing(element, text: str) -> None:
    if not ENABLE_STEALTH:
        element.send_keys(text)
        return
    for char in text:
        element.send_keys(char)
        time.sleep(random.uniform(0.05, 0.15))

def simulate_scroll(driver, scroll_count: int = 3) -> None:
    if not ENABLE_STEALTH or not SCROLL_BEHAVIOR: return
    for _ in range(scroll_count):
        scroll_amount = random.randint(200, 500)
        driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.3, 0.8))

def get_random_user_agent() -> str:
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]
    return random.choice(user_agents)

def inject_stealth_scripts(driver) -> None:
    if not ENABLE_STEALTH: return
    stealth_js = """
    Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
    window.chrome = { runtime: {} };
    const originalQuery = window.navigator.permissions.query;
    window.navigator.permissions.query = (parameters) => (
        parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
    );
    Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
    Object.defineProperty(navigator, 'languages', { get: () => ['ko-KR', 'ko', 'en-US', 'en'] });
    """
    try:
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {'source': stealth_js})
        logging.info("ğŸ”’ ìŠ¤í…”ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì… ì™„ë£Œ")
    except Exception as e:
        logging.warning(f"âš ï¸ ìŠ¤í…”ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì… ì‹¤íŒ¨: {e}")

# =============================================================================
# 3) í”„ë¡ì‹œ ìˆ˜ì§‘ ë° ê²€ì¦ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================
def fetch_all_proxies() -> List[ProxyInfo]:
    logging.info("ğŸ“„ [ìˆ˜ì§‘] í”„ë¡ì‹œ ìˆ˜ì§‘ ì‹œì‘")
    raw_list = []
    for url, default_proto, _ in ALL_SOURCES:
        if STOP_EVENT.is_set(): break
        try:
            resp = requests.get(url, timeout=20, headers=HEADERS)
            if resp.status_code != 200: continue
            content = resp.text.strip()
            count = 0
            if content.startswith('{') or content.startswith('['):
                try:
                    data = resp.json()
                    items = data.get('data', []) if isinstance(data, dict) else data
                    for item in items:
                        if isinstance(item, dict) and 'ip' in item and 'port' in item:
                            addr = f"{item['ip']}:{item['port']}"
                            actual_proto = item['protocols'][0].lower() if 'protocols' in item and item['protocols'] else default_proto
                            raw_list.append(ProxyInfo(protocol=actual_proto, address=addr, source=urlparse(url).netloc))
                            count += 1
                except: pass
            else:
                for line in content.splitlines():
                    line = line.strip()
                    if not line or line.startswith('#'): continue
                    addr = line.split("://")[-1] if "://" in line else line
                    if ":" in addr:
                        raw_list.append(ProxyInfo(protocol=default_proto, address=addr, source=urlparse(url).netloc))
                        count += 1
            if count > 0:
                logging.info(f"ğŸ“¥ [ìˆ˜ì§‘] {urlparse(url).netloc:20s} | {count:4d}ê°œ ({default_proto})")
        except Exception as e:
            logging.error(f"âš ï¸ [ì‹¤íŒ¨] {urlparse(url).netloc}: {e}")
    uniq = {(p.protocol, p.address): p for p in raw_list}
    proxies = list(uniq.values())
    logging.info(f"ğŸ“Š [ìµœì¢…] ì´ {len(proxies)}ê°œì˜ ê³ ìœ  í”„ë¡ì‹œ ë¡œë“œ ì™„ë£Œ")
    return proxies

def tcp_quick_check(addr: str, timeout: float = 2.0) -> bool:
    try:
        host, port_s = addr.split(":", 1)
        port = int(port_s)
        with socket.create_connection((host, port), timeout=timeout): return True
    except Exception: return False

# =============================================================================
# 4) ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ìƒì„± (ê¸°ì¡´ ìœ ì§€)
# =============================================================================
def make_driver(proxy: Optional[ProxyInfo], slot_id: str = "0") -> Tuple[uc.Chrome, str]:
    tmp_root = Path(__file__).resolve().parent / "_tmp_profiles"
    tmp_root.mkdir(parents=True, exist_ok=True)
    profile_dir = tempfile.mkdtemp(prefix=f"naver_mon_profile_", dir=str(tmp_root))
    
    options = uc.ChromeOptions()
    options.add_argument(f"--user-data-dir={profile_dir}")
    options.add_argument("--no-first-run")
    options.add_argument("--no-default-browser-check")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-notifications")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--lang=ko-KR")
    
    if ENABLE_STEALTH:
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-web-security")
        options.add_argument("--disable-features=IsolateOrigins,site-per-process")
        options.add_argument("--disable-infobars")
        options.add_argument("--disable-extensions")
        options.add_argument("--profile-directory=Default")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--disable-gpu")
        user_agent = get_random_user_agent()
        options.add_argument(f"--user-agent={user_agent}")
        logging.info(f"ğŸ­ ëœë¤ User-Agent ì ìš©: {user_agent[:50]}...")
    
    if RUN_HEADLESS: options.add_argument("--headless=new")
    if proxy:
        proxy_str = f"{proxy.protocol}://{proxy.address}"
        logging.info(f"ğŸŒ [ë“œë¼ì´ë²„ ìƒì„±] í”„ë¡ì‹œ ì ìš©: {proxy_str} (ì¶œì²˜: {proxy.source})")
        options.add_argument(f"--proxy-server={proxy_str}")

    driver = uc.Chrome(options=options, use_subprocess=True)
    driver.set_page_load_timeout(PAGELOAD_TIMEOUT_SEC)
    
    if ENABLE_STEALTH: inject_stealth_scripts(driver)
    
    saved_state = load_window_state(slot_id)
    try:
        if saved_state:
            driver.set_window_size(saved_state['width'], saved_state['height'])
            driver.set_window_position(saved_state['x'], saved_state['y'])
        else:
            if ENABLE_WINDOW_SIZE:
                w, h = WINDOW_WIDTH, WINDOW_HEIGHT
                if ENABLE_WINDOW_JITTER:
                    w += random.randint(-WINDOW_JITTER_RANGE, WINDOW_JITTER_RANGE)
                    h += random.randint(-WINDOW_JITTER_RANGE, WINDOW_JITTER_RANGE)
                driver.set_window_size(max(300, w), max(300, h))
            if ENABLE_WINDOW_POSITION:
                driver.set_window_position(WINDOW_POS_X, WINDOW_POS_Y)
    except Exception as e:
        logging.warning(f"âš ï¸ ì°½ ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
    return driver, profile_dir

def update_query_param(url: str, **kwargs) -> str:
    u = urlparse(url)
    q = parse_qs(u.query)
    for k, v in kwargs.items(): q[str(k)] = [str(v)]
    return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(q, doseq=True), u.fragment))

# =============================================================================
# 5) ì‘ì—… ë¡œì§ (IP ë…¸ì¶œ í•„í„°ë§ ê¸°ëŠ¥ í†µí•©)
# =============================================================================
def thread_worker(task: Dict, proxy: ProxyInfo, slot_id: str = "0"):
    keyword, target_url = task["keyword"], task["domain"]
    logging.info(f"â–¶ï¸ ì‘ì—… ì‹œì‘ | ìŠ¬ë¡¯: {slot_id} | í‚¤ì›Œë“œ: [{keyword}] | í”„ë¡ì‹œ: {proxy.address}")
    
    driver, profile_dir = None, ""
    rr = RunResult(datetime.now().isoformat(timespec="seconds"), keyword, target_url, proxy.protocol, proxy.address, proxy.source, False, None, None, None, False, None, None, None)
    
    try:
        # 1. TCP ì²´í¬ ë° ë‚´ IP ìœ ì¶œ ê²€ì‚¬
        if not tcp_quick_check(proxy.address):
            logging.warning(f"âŒ TCP ì—°ê²° ì‹¤íŒ¨: {proxy.address}")
            rr.error = "TCP_CONNECT_FAIL"
        elif is_proxy_leaking_my_ip(proxy, MY_PUBLIC_IP):
            logging.warning(f"âŒ í”„ë¡ì‹œ ê±°ë¶€ (ë‚´ ê³µì¸ IP ë…¸ì¶œë¨): {proxy.address}")
            rr.error = "IP_LEAK_DETECTED"
        else:
            logging.info(f"ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘... (ìŠ¬ë¡¯ {slot_id})")
            driver, profile_dir = make_driver(proxy, slot_id)
            
            random_delay(1.0, 2.0)
            logging.info(f"ğŸ” ë„¤ì´ë²„ ì ‘ì† ë° í‚¤ì›Œë“œ ê²€ìƒ‰: [{keyword}]")
            driver.get("https://www.naver.com/")
            WebDriverWait(driver, ELEM_WAIT_SEC).until(lambda d: d.execute_script("return document.readyState") in ("interactive", "complete"))
            
            random_delay(1.5, 3.0)
            simulate_scroll(driver, scroll_count=2)
            
            box = WebDriverWait(driver, ELEM_WAIT_SEC).until(EC.presence_of_element_located((By.NAME, "query")))
            box.clear()
            simulate_human_typing(box, keyword)
            random_delay(0.5, 1.0)
            box.send_keys(Keys.ENTER)
            
            WebDriverWait(driver, ELEM_WAIT_SEC).until(lambda d: "search.naver.com" in (d.current_url or ""))
            results_url = driver.current_url
            random_delay(2.0, 4.0)
            
            for page in range(1, MAX_PAGES + 1):
                if STOP_EVENT.is_set(): break
                logging.info(f"ğŸ“„ í˜ì´ì§€ íƒìƒ‰ ì¤‘... ({page}/{MAX_PAGES} page)")
                driver.get(update_query_param(results_url, start=1 + (page - 1) * 10))
                random_delay(2.0, 3.5)
                simulate_scroll(driver, scroll_count=3)
                
                found_data = None
                anchors = driver.find_elements(By.CSS_SELECTOR, "a[href]")
                for idx, a in enumerate(anchors, 1):
                    try:
                        href = a.get_attribute("href") or ""
                        if href and target_url:
                            h_can = urlunparse((urlparse(href).scheme, urlparse(href).netloc, urlparse(href).path or "/", "", "", ""))
                            t_can = urlunparse((urlparse(target_url).scheme, urlparse(target_url).netloc, urlparse(target_url).path or "/", "", "", ""))
                            if h_can.lower() == t_can.lower():
                                found_data = (idx, href)
                                break
                    except: continue
                
                if found_data:
                    rank, href = found_data
                    rr.found, rr.found_page, rr.found_rank_on_page, rr.found_href = True, page, rank, href
                    random_delay(1.0, 2.5)
                    driver.get(href)
                    WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") in ("interactive", "complete"))
                    random_delay(2.0, 3.0)
                    
                    final_url = driver.current_url
                    h_final = urlunparse((urlparse(final_url).scheme, urlparse(final_url).netloc, urlparse(final_url).path or "/", "", "", ""))
                    if h_final.lower() == t_can.lower():
                        rr.clicked_ok, rr.final_url = True, final_url
                    else:
                        rr.clicked_ok, rr.final_url, rr.note = False, final_url, "FINAL_URL_NOT_MATCH"
                    break
                
                if page < MAX_PAGES: random_delay(1.5, 3.0)
            
            if not rr.found and not rr.error:
                rr.error = "NOT_FOUND_IN_PAGES"

    except Exception as e:
        logging.error(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {str(e)[:100]}")
        rr.error = str(e)[:160]
    finally:
        if driver:
            try:
                pos = driver.get_window_position()
                size = driver.get_window_size()
                save_window_state(slot_id, pos['x'], pos['y'], size['width'], size['height'])
            except: pass
            try: driver.quit()
            except: pass
        if profile_dir: shutil.rmtree(profile_dir, ignore_errors=True)
        
        with FILE_LOCK:
            with open(RESULT_JSONL, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(rr), ensure_ascii=False) + "\n")
            is_new = not os.path.exists(RESULT_CSV)
            with open(RESULT_CSV, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                if is_new: w.writerow(["ts", "keyword", "target_url", "proxy_protocol", "proxy_address", "proxy_source", "found", "found_page", "found_rank_on_page", "found_href", "clicked_ok", "final_url", "error", "note"])
                w.writerow([rr.ts, rr.keyword, rr.target_url, rr.proxy_protocol, rr.proxy_address, rr.proxy_source, rr.found, rr.found_page, rr.found_rank_on_page, rr.found_href, rr.clicked_ok, rr.final_url, rr.error, rr.note])
        logging.info(f"ğŸ ì‘ì—… ì¢…ë£Œ | ìŠ¬ë¡¯: {slot_id} | ê²°ê³¼: {'ì„±ê³µ' if rr.found else 'ì‹¤íŒ¨'}")

# =============================================================================
# 6) ë©”ì¸ ë£¨í”„
# =============================================================================
def main_loop() -> None:
    global MY_PUBLIC_IP
    setup_logging()
    logging.info("==================================================")
    logging.info("ğŸš€ Naver Exposure Monitor ì‹œì‘")
    
    # ì‹œì‘ ì‹œ ë‚´ ì‹¤ì œ ê³µì¸ IPë¥¼ ë¨¼ì € í™•ì¸
    MY_PUBLIC_IP = get_my_actual_ip()
    logging.info(f"ğŸ  ë‚´ ê³µì¸ IP: {MY_PUBLIC_IP}")
    
    logging.info(f"âš™ï¸ ì„¤ì •: ì“°ë ˆë“œ ìŠ¬ë¡¯ {MAX_THREADS}ê°œ / íƒìƒ‰ {MAX_PAGES}í˜ì´ì§€")
    logging.info("==================================================")
    
    proxies_cache = []
    active_threads: List[threading.Thread] = []
    
    try:
        while not STOP_EVENT.is_set():
            if REFRESH_PROXIES_EACH_CYCLE or not proxies_cache: 
                proxies_cache = fetch_all_proxies()
            
            if not proxies_cache:
                logging.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. 60ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                time.sleep(60); continue

            for task in TASKS:
                for idx, proxy in enumerate(proxies_cache):
                    if STOP_EVENT.is_set(): break
                    while len(active_threads) >= MAX_THREADS:
                        active_threads = [t for t in active_threads if t.is_alive()]
                        time.sleep(1)
                    
                    used_slots = set()
                    for t in active_threads:
                        if t.is_alive() and '-slot' in t.name:
                            try: used_slots.add(int(t.name.split('-slot')[-1]))
                            except: pass
                    
                    available_slot = None
                    for slot_num in range(MAX_THREADS):
                        if slot_num not in used_slots:
                            available_slot = slot_num
                            break
                    if available_slot is None: available_slot = 0
                    
                    slot_id = str(available_slot)
                    t_name = f"{task['keyword']}-{idx}-slot{slot_id}"
                    t = threading.Thread(target=thread_worker, args=(task, proxy, slot_id), name=t_name, daemon=True)
                    active_threads.append(t)
                    t.start()
                    logging.info(f"â• ìƒˆ ì“°ë ˆë“œ í• ë‹¹: [{t_name}]")

            while any(t.is_alive() for t in active_threads):
                active_threads = [t for t in active_threads if t.is_alive()]
                time.sleep(2)
            logging.info(f"âœ… ì‚¬ì´í´ ì™„ë£Œ. {CHECK_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°...")
            time.sleep(CHECK_INTERVAL_SECONDS)
            
    except KeyboardInterrupt:
        STOP_EVENT.set()
        logging.info("ğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

if __name__ == "__main__":
    main_loop()