import os
import re
import json
import time
import csv
import socket
import shutil
import random
import logging
import tempfile
import threading
import struct
import redis
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from pathlib import Path

import requests
from DrissionPage.common import Keys
from playwright.sync_api import sync_playwright

# stealth_browser.pyì—ì„œ í´ë˜ìŠ¤ ì„í¬íŠ¸
from stealth_browser import StealthMobileBrowser

# =============================================================================
# 0) ì‚¬ìš©ì ì„¤ì •
# =============================================================================
MAX_THREADS = 1

ENABLE_WINDOW_SIZE = True
WINDOW_WIDTH = 1000
WINDOW_HEIGHT = 400
ENABLE_WINDOW_JITTER = False
WINDOW_JITTER_RANGE = 80
ENABLE_WINDOW_POSITION = True
WINDOW_POS_X = 50
WINDOW_POS_Y = 400
ENABLE_BLOCK_CHECK = False
CHECK_INTERVAL_SECONDS = 60
MAX_PAGES = 10

TASKS = [
    #{"keyword": "ì˜¬ë¹¼ë¯¸í‹°ë¹„", "domain": "https://www.tvda.co.kr/?srt=1"},
    {"keyword": "ë¸”ë‘í‹°ë¹„", "domain": "https://www.flyingobjectives.co.kr/rank/"},
]

MAX_PROXIES_PER_TASK = 30
REFRESH_PROXIES_EACH_CYCLE = True
PAGELOAD_TIMEOUT_SEC = 60*2
ELEM_WAIT_SEC = 30

# ğŸ”’ ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì„¤ì •
ENABLE_STEALTH = True
RANDOM_DELAY_MIN = 2.0
RANDOM_DELAY_MAX = 5.0
ENABLE_MOUSE_MOVEMENT = True
SCROLL_BEHAVIOR = True

OUT_DIR = os.path.abspath("./naver_monitor_out")
LOG_FILE = os.path.join(OUT_DIR, "monitor.log")
RESULT_JSONL = os.path.join(OUT_DIR, "results.jsonl")
RESULT_CSV = os.path.join(OUT_DIR, "results.csv")
WINDOW_STATE_FILE = os.path.join(OUT_DIR, "window_states.json")
STOP_EVENT = threading.Event()
FILE_LOCK = threading.Lock()
WINDOW_STATE_LOCK = threading.Lock()

# ì „ì—­ ë³€ìˆ˜
MY_PUBLIC_IP = None

# Redis ì„¤ì •
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_ZSET_ALIVE = "proxies:alive"
REDIS_ZSET_LEASE = "proxies:lease"

# Lua ìŠ¤í¬ë¦½íŠ¸ (í”„ë¡ì‹œ ì„ëŒ€)
_LUA_CLAIM = """
local alive_key = KEYS[1]
local lease_key = KEYS[2]
local now = tonumber(ARGV[1])
local lease_time = tonumber(ARGV[2])

local members = redis.call('ZRANGEBYSCORE', alive_key, 0, now)
if #members > 0 then
    local proxy = members[1]
    redis.call('ZREM', alive_key, proxy)
    redis.call('ZADD', lease_key, now + lease_time, proxy)
    return proxy
end
return nil
"""

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

# =============================================================================
# 1) Playwright ë””ë°”ì´ìŠ¤ ë¡œë“œ
# =============================================================================
def get_playwright_devices():
    print("ğŸŒ Playwright ê¸°ê¸° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ì¤‘...")
    out = {}
    with sync_playwright() as p:
        for name, spec in p.devices.items():
            is_mobile = spec.get("is_mobile", spec.get("isMobile", False))
            if not is_mobile:
                continue
            
            if "landscape" in name.lower():
                continue

            user_agent = spec.get("user_agent", spec.get("userAgent"))
            viewport = spec.get("viewport")
            dsf = spec.get("device_scale_factor", spec.get("deviceScaleFactor", 2))
            has_touch = spec.get("has_touch", spec.get("hasTouch", True))

            if not user_agent or not viewport:
                continue

            out[name] = {
                "user_agent": user_agent,
                "viewport": viewport,
                "device_pixel_ratio": dsf,
                "has_touch": has_touch,
            }

    print(f"âœ… Playwright ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ë¡œë“œ: {len(out)}ê°œ")
    if out:
        sample_name = next(iter(out.keys()))
        print(f"ğŸ” ìƒ˜í”Œ ë””ë°”ì´ìŠ¤: {sample_name}")
    return out

PLAYWRIGHT_DEVICES = get_playwright_devices()

# =============================================================================
# 2) ì§€ì—­ í”„ë¡œí•„ ë¡œë“œ
# =============================================================================
REGION_PROFILES = {}
try:
    if os.path.exists(r".\DrissionPage\region_profiles_mobile.json"):
        with open(r".\DrissionPage\region_profiles_mobile.json", 'r', encoding='utf-8') as f:
            REGION_PROFILES = json.load(f)
        print(f"âœ… ì§€ì—­ í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ ({len(REGION_PROFILES)}ê°œ ì§€ì—­)")
    else:
        print("âš ï¸ .\DrissionPage\region_profiles_mobile.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        REGION_PROFILES = {
            "KR": {
                "locale": "ko-KR",
                "timezone": "Asia/Seoul",
                "referers": ["https://www.naver.com/", "https://www.google.com/"]
            }
        }
except Exception as e:
    print(f"âŒ ì§€ì—­ í”„ë¡œí•„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    REGION_PROFILES = {
        "KR": {
            "locale": "ko-KR",
            "timezone": "Asia/Seoul",
            "referers": ["https://www.naver.com/"]
        }
    }

# =============================================================================
# 3) ë°ì´í„° ëª¨ë¸ ë° ìœ í‹¸
# =============================================================================
@dataclass
class ProxyInfo:
    protocol: str
    address: str
    source: str

@dataclass
class RunResult:
    ts: str
    keyword: str
    target_url: str
    proxy_protocol: Optional[str]
    proxy_address: Optional[str]
    proxy_source: Optional[str]
    found: bool
    found_page: Optional[int]
    found_rank_on_page: Optional[int]
    found_href: Optional[str]
    clicked_ok: bool
    final_url: Optional[str]
    error: Optional[str]
    note: Optional[str]

def setup_logging() -> None:
    os.makedirs(OUT_DIR, exist_ok=True)
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s", datefmt="%H:%M:%S")
    
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    fh.setFormatter(fmt)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    
    logger.handlers.clear()
    logger.addHandler(fh)
    logger.addHandler(sh)

# =============================================================================
# 4) IP í™•ì¸ ë° ìŠ¤í…”ìŠ¤ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
def get_my_actual_ip():
    """ì‹¤í–‰ ì‹œì ì˜ ë‚´ ì‹¤ì œ ê³µì¸ IP í™•ì¸"""
    try:
        res = requests.get("https://api.ipify.org", timeout=10)
        return res.text.strip()
    except:
        return None

def is_proxy_leaking_my_ip(proxy: ProxyInfo, my_ip: str):
    """í”„ë¡ì‹œê°€ ë‚´ IPë¥¼ ìœ ì¶œí•˜ê³  ìˆëŠ”ì§€ í™•ì¸"""
    if not my_ip:
        return False
    try:
        proxies = {
            "http": f"{proxy.protocol}://{proxy.address}",
            "https": f"{proxy.protocol}://{proxy.address}"
        }
        res = requests.get("https://api.ipify.org", proxies=proxies, timeout=10)
        return res.text.strip() == my_ip
    except:
        return False

def load_window_state(slot_id: str) -> Optional[Dict]:
    try:
        with WINDOW_STATE_LOCK:
            if os.path.exists(WINDOW_STATE_FILE):
                with open(WINDOW_STATE_FILE, 'r', encoding='utf-8') as f:
                    states = json.load(f)
                    return states.get(slot_id)
    except Exception as e:
        logging.warning(f"âš ï¸ ì°½ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨ (ìŠ¬ë¡¯ {slot_id}): {e}")
    return None

def save_window_state(slot_id: str, x: int, y: int, width: int, height: int) -> None:
    try:
        with WINDOW_STATE_LOCK:
            states = {}
            if os.path.exists(WINDOW_STATE_FILE):
                with open(WINDOW_STATE_FILE, 'r', encoding='utf-8') as f:
                    states = json.load(f)
            states[slot_id] = {'x': x, 'y': y, 'width': width, 'height': height}
            with open(WINDOW_STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(states, f, indent=2)
            logging.info(f"ğŸ’¾ ì°½ ìƒíƒœ ì €ì¥ ì™„ë£Œ (ìŠ¬ë¡¯ {slot_id}): ìœ„ì¹˜({x},{y}) í¬ê¸°({width}x{height})")
    except Exception as e:
        logging.warning(f"âš ï¸ ì°½ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨ (ìŠ¬ë¡¯ {slot_id}): {e}")

def random_delay(min_sec: float = None, max_sec: float = None) -> None:
    if not ENABLE_STEALTH:
        return
    min_val = min_sec if min_sec is not None else RANDOM_DELAY_MIN
    max_val = max_sec if max_sec is not None else RANDOM_DELAY_MAX
    time.sleep(random.uniform(min_val, max_val))

def simulate_human_typing(element, text: str) -> None:
    """DrissionPage ìš”ì†Œì— ëŒ€í•œ ì¸ê°„ íƒ€ì´í•‘ ì‹œë®¬ë ˆì´ì…˜ (ëª¨ë°”ì¼ ì›¹ UI ë³€ê²½ ëŒ€ì‘)"""

    try:
        # 1. ê²€ìƒ‰ì°½ í´ë¦­ (UI ë³€ê²½ íŠ¸ë¦¬ê±°)
        logging.debug("ğŸ” ê²€ìƒ‰ì°½ í´ë¦­ ì‹œë„...")
        try:
            element.click()
            logging.debug("âœ… ê²€ìƒ‰ì°½ í´ë¦­ ì™„ë£Œ")
        except:
            # í´ë¦­ ì‹¤íŒ¨ ì‹œ JSë¡œ ì‹œë„
            try:
                element.run_js("this.click();")
                logging.debug("âœ… ê²€ìƒ‰ì°½ í´ë¦­ ì™„ë£Œ (JS)")
            except Exception as e:
                logging.warning(f"âš ï¸ ê²€ìƒ‰ì°½ í´ë¦­ ì‹¤íŒ¨: {str(e)[:100]}")
        
        # 2. UI ë³€ê²½ ëŒ€ê¸° (ì¤‘ìš”!)
        time.sleep(random.uniform(0.8, 1.5))
        logging.debug("â³ UI ë³€ê²½ ëŒ€ê¸° ì™„ë£Œ")
        
        # 3. ë³€ê²½ëœ UIì—ì„œ í™œì„±í™”ëœ ì…ë ¥ì°½ ì°¾ê¸°
        page = element.page
        active_input = None
        
        # ë°©ë²• 1: í¬ì»¤ìŠ¤ëœ ìš”ì†Œ ì°¾ê¸°
        try:
            active_input = page.run_js("return document.activeElement;")
            if active_input and active_input.tag in ['input', 'textarea']:
                logging.debug("âœ… í¬ì»¤ìŠ¤ëœ ì…ë ¥ì°½ ë°œê²¬ (activeElement)")
            else:
                active_input = None
        except:
            pass
        
        # ë°©ë²• 2: name='query'ì¸ visible ì…ë ¥ì°½ ì°¾ê¸°
        if not active_input:
            try:
                inputs = page.eles("@name=query")
                for inp in inputs:
                    try:
                        # í™”ë©´ì— ë³´ì´ëŠ” ì…ë ¥ì°½ì¸ì§€ í™•ì¸
                        is_visible = page.run_js("""
                            var elem = arguments[0];
                            return elem.offsetWidth > 0 && 
                                   elem.offsetHeight > 0 && 
                                   window.getComputedStyle(elem).visibility !== 'hidden' &&
                                   window.getComputedStyle(elem).display !== 'none';
                        """, inp)
                        if is_visible:
                            active_input = inp
                            logging.debug("âœ… ë³´ì´ëŠ” ì…ë ¥ì°½ ë°œê²¬ (name=query)")
                            break
                    except:
                        continue
            except:
                pass
        
        # ë°©ë²• 3: CSS selectorë¡œ visible input ì°¾ê¸°
        if not active_input:
            try:
                inputs = page.eles("css:input[type='text'], css:input[type='search'], css:input:not([type])")
                for inp in inputs:
                    try:
                        is_visible = page.run_js("""
                            var elem = arguments[0];
                            var rect = elem.getBoundingClientRect();
                            return rect.width > 0 && 
                                   rect.height > 0 && 
                                   window.getComputedStyle(elem).visibility !== 'hidden' &&
                                   window.getComputedStyle(elem).display !== 'none';
                        """, inp)
                        if is_visible:
                            active_input = inp
                            logging.debug("âœ… ë³´ì´ëŠ” ì…ë ¥ì°½ ë°œê²¬ (CSS selector)")
                            break
                    except:
                        continue
            except:
                pass
        
        # 4. ì…ë ¥ì°½ì„ ëª» ì°¾ì•˜ìœ¼ë©´ ì›ë˜ element ì‚¬ìš©
        if not active_input:
            logging.warning("âš ï¸ í™œì„± ì…ë ¥ì°½ì„ ì°¾ì§€ ëª»í•¨, ì›ë˜ element ì‚¬ìš©")
            active_input = element
        
        # 5. ì…ë ¥ì°½ì— í¬ì»¤ìŠ¤
        try:
            active_input.click()
            time.sleep(random.uniform(0.2, 0.4))
        except:
            try:
                page.run_js("arguments[0].focus();", active_input)
                time.sleep(random.uniform(0.2, 0.4))
            except:
                pass
        
        # 6. ê¸°ì¡´ í…ìŠ¤íŠ¸ í´ë¦¬ì–´
        try:
            active_input.clear()
            time.sleep(random.uniform(0.1, 0.2))
        except:
            # clear ì‹¤íŒ¨ ì‹œ JSë¡œ ì‹œë„
            try:
                page.run_js("arguments[0].value = '';", active_input)
                time.sleep(random.uniform(0.1, 0.2))
            except:
                pass
        
        # 7. í…ìŠ¤íŠ¸ ì…ë ¥
        logging.debug(f"âŒ¨ï¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì‹œì‘: {text}")
        if not ENABLE_STEALTH:
            active_input.input(text)
        else:
            # ìŠ¤í…”ìŠ¤ ëª¨ë“œ: í•œ ê¸€ìì”© ì…ë ¥
            for char in text:
                active_input.input(char)
                time.sleep(random.uniform(0.05, 0.15))
        
        # 8. ì…ë ¥ ì™„ë£Œ í›„ ëŒ€ê¸°
        time.sleep(random.uniform(0.3, 0.6))
        logging.debug(f"âœ… í…ìŠ¤íŠ¸ ì…ë ¥ ì™„ë£Œ: {text}")
        
    except Exception as e:
        logging.error(f"âŒ íƒ€ì´í•‘ ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜: {str(e)[:200]}")
        import traceback
        logging.debug(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìµœì†Œí•œ í…ìŠ¤íŠ¸ëŠ” ì…ë ¥ ì‹œë„
        try:
            element.input(text)
        except:
            pass

def simulate_scroll(page, scroll_count: int = 3) -> None:
    """DrissionPage í˜ì´ì§€ ìŠ¤í¬ë¡¤"""
    if not ENABLE_STEALTH or not SCROLL_BEHAVIOR:
        return
    for _ in range(scroll_count):
        scroll_amount = random.randint(200, 500)
        page.run_js(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.3, 0.8))

def simulate_natural_scroll(page, min_actions: int = 6, max_actions: int = 12) -> None:
    """ìì—°ìŠ¤ëŸ¬ìš´ ì½ê¸° í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
    if not ENABLE_STEALTH or not SCROLL_BEHAVIOR:
        return

    try:
        scroll_h = page.run_js(
            "return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight) || 0;"
        )
        view_h = page.run_js("return window.innerHeight || 0;")
        if not scroll_h or not view_h or scroll_h <= view_h + 80:
            return
    except Exception:
        return

    actions = random.randint(min_actions, max_actions)
    down_actions = max(2, int(actions * random.uniform(0.6, 0.8)))
    up_actions = max(1, actions - down_actions)

    # ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
    for _ in range(down_actions):
        step = random.randint(int(view_h * 0.25), int(view_h * 0.95))
        try:
            page.run_js(
                "window.scrollBy({top: arguments[0], left: 0, behavior: 'smooth'});",
                step,
            )
        except Exception:
            page.run_js("window.scrollBy(0, arguments[0]);", step)
        time.sleep(random.uniform(0.4, 1.2))
        if random.random() < 0.25:
            time.sleep(random.uniform(0.7, 1.8))

    time.sleep(random.uniform(1.0, 2.5))

    # ìœ„ë¡œ ë˜ëŒë¦¬ê¸°
    for _ in range(up_actions):
        step = random.randint(int(view_h * 0.15), int(view_h * 0.75))
        try:
            page.run_js(
                "window.scrollBy({top: -arguments[0], left: 0, behavior: 'smooth'});",
                step,
            )
        except Exception:
            page.run_js("window.scrollBy(0, -arguments[0]);", step)
        time.sleep(random.uniform(0.35, 1.0))

    if random.random() < 0.5:
        jiggle = random.randint(-120, 120)
        page.run_js("window.scrollBy(0, arguments[0]);", jiggle)
        time.sleep(random.uniform(0.2, 0.6))

# =============================================================================
# 5) Redis í”„ë¡ì‹œ ê´€ë¦¬
# =============================================================================
def fetch_proxies_from_redis(r: redis.Redis, max_count: int = 100) -> List[ProxyInfo]:
    """Redisì—ì„œ í”„ë¡ì‹œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    logging.info(f"ğŸ”„ [ìˆ˜ì§‘] Redisì—ì„œ í”„ë¡ì‹œ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_count}ê°œ)")
    proxies = []
    
    try:
        # alive í‚¤ì—ì„œ í”„ë¡ì‹œ ê°€ì ¸ì˜¤ê¸°
        now = int(time.time())
        members = r.zrangebyscore(REDIS_ZSET_ALIVE, 0, now, start=0, num=max_count)
        
        for proxy_str in members:
            try:
                # í”„ë¡ì‹œ í˜•ì‹ íŒŒì‹± (ì˜ˆ: "http://user:pass@ip:port" ë˜ëŠ” "ip:port")
                if "://" in proxy_str:
                    protocol = proxy_str.split("://")[0]
                    address = proxy_str.split("://")[1]
                else:
                    protocol = "http"
                    address = proxy_str
                
                proxies.append(ProxyInfo(
                    protocol=protocol,
                    address=address,
                    source="redis"
                ))
            except Exception as e:
                logging.warning(f"âš ï¸ í”„ë¡ì‹œ íŒŒì‹± ì‹¤íŒ¨: {proxy_str} | {e}")
                continue
        
        logging.info(f"ğŸ“Š [ìµœì¢…] Redisì—ì„œ {len(proxies)}ê°œì˜ í”„ë¡ì‹œ ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        logging.error(f"âŒ Redis í”„ë¡ì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    return proxies

def return_proxy_to_redis(r: redis.Redis, proxy: ProxyInfo):
    """í”„ë¡ì‹œë¥¼ Redisì— ë°˜ë‚©"""
    try:
        proxy_str = f"{proxy.protocol}://{proxy.address}"
        r.zrem(REDIS_ZSET_LEASE, proxy_str)
        r.zadd(REDIS_ZSET_ALIVE, {proxy_str: int(time.time()) + 60})
        logging.info(f"ğŸ”„ í”„ë¡ì‹œ ë°˜ë‚©: {proxy_str}")
    except Exception as e:
        logging.warning(f"âš ï¸ í”„ë¡ì‹œ ë°˜ë‚© ì‹¤íŒ¨: {e}")

def tcp_quick_check(addr: str, timeout: float = 2.0) -> bool:
    return True
    try:
        host, port_s = addr.split(":", 1)
        port = int(port_s)
        with socket.create_connection((host, port), timeout=timeout):
            return True
    except Exception:
        return False

# =============================================================================
# 6) ë¸Œë¼ìš°ì € ìƒì„± (StealthMobileBrowser ì‚¬ìš©)
# =============================================================================
def make_stealth_browser(proxy: Optional[ProxyInfo], slot_id: str = "0") -> Tuple[any, any]:
    """StealthMobileBrowserë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì € ìƒì„±"""
    try:
        # í”„ë¡œí•„ ì„ íƒ
        region_key = random.choice(list(REGION_PROFILES.keys()))
        profile = REGION_PROFILES[region_key]
        selected_referer = random.choice(profile.get("referers", ["https://www.naver.com/"]))
        
        logging.info(f"ğŸŒ ì§€ì—­: {region_key} | ìœ ì…ê²½ë¡œ: {selected_referer}")
        
        # í”„ë¡ì‹œ ë¬¸ìì—´ ìƒì„±
        proxy_str = None
        if proxy:
            proxy_str = f"{proxy.protocol}://{proxy.address}"
            logging.info(f"ğŸŒ [ë¸Œë¼ìš°ì € ìƒì„±] í”„ë¡ì‹œ ì ìš©: {proxy_str} (ì¶œì²˜: {proxy.source})")
        
        # StealthMobileBrowser ìƒì„±
        browser_wrapper = StealthMobileBrowser(
            slot_index=int(slot_id),
            profile=profile,
            proxy=proxy_str,
            devices_dict=PLAYWRIGHT_DEVICES,
            referer=selected_referer
        )
        
        page = browser_wrapper.page
        
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (DrissionPageëŠ” set.timeouts ë©”ì„œë“œ ì‚¬ìš©)
        try:
            page.set.timeouts(base=PAGELOAD_TIMEOUT_SEC, page_load=PAGELOAD_TIMEOUT_SEC)
        except:
            pass
        
        logging.info(f"âœ¨ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¬ë¡¯ {slot_id})")
        
        return browser_wrapper, page

    except Exception as e:
        logging.error(f"ğŸ›‘ make_stealth_browser ì˜ˆì™¸: {e}")
        raise

def update_query_param(url: str, **kwargs) -> str:
    u = urlparse(url)
    q = parse_qs(u.query)
    for k, v in kwargs.items():
        q[str(k)] = [str(v)]
    return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(q, doseq=True), u.fragment))

def wait_and_mouse_click_live_more(page, timeout=20):
    # 1) í–„ë²„ê±°(ë©”ë‰´) ì•„ì´ì½˜: ì‚¬ëŒ í´ë¦­
    menu = page.ele("css:img[src*='menu_ham.svg']", timeout=timeout)
    if not menu:
        logging.warning("ë©”ë‰´ ì•„ì´ì½˜ ëª» ì°¾ìŒ")
        return False

    # element.click / JS click ë§ê³  actionsë¡œ í´ë¦­
    page.actions.move_to(menu).click().perform()

    # 2) ë ˆì´ì–´ ëœ° ë•Œê¹Œì§€ /live-more ë§í¬ê°€ "ë³´ì¼ ë•Œ"ê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì‚¬ëŒ í´ë¦­
    end = time.time() + timeout
    live = None
    while time.time() < end:
        live = page.ele("css:a[href='/live-more'], css:a[href*='live-more']", timeout=1)
        if live and live.is_displayed():  # DrissionPage ìš”ì†Œ ê°€ì‹œì„± ì²´í¬ :contentReference[oaicite:0]{index=0}
            break
        time.sleep(0.1)

    if not live:
        logging.warning("/live-more ë§í¬ ëª» ì°¾ìŒ(ë©”ë‰´ê°€ ì•ˆ ì—´ë ¸ê±°ë‚˜ DOMì´ ë‹¤ë¦„)")
        return False

    page.actions.move_to(live).click().perform()
    return True

def wait_and_mouse_click_live_more_old(page, timeout=60):
    """ë©”ë‰´ ë²„íŠ¼ í´ë¦­ í›„ live-more ë§í¬ í´ë¦­"""
    try:
        # 1ë‹¨ê³„: ë©”ë‰´ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
        logging.info("ğŸ” [1ë‹¨ê³„] ë©”ë‰´ ë²„íŠ¼ ì°¾ê¸° ì‹œì‘...")
        
        # ë©”ë‰´ ë²„íŠ¼ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        menu_button = None
        
        # ë°©ë²• 1: alt ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
        try:
            menu_button = page.ele("css:img[alt='ë©”ë‰´ ë²„íŠ¼']", timeout=5)
            if menu_button:
                logging.info("âœ… ë©”ë‰´ ë²„íŠ¼ ë°œê²¬ (ë°©ë²• 1: alt ì†ì„±)")
        except:
            pass
        
        # ë°©ë²• 2: src ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
        if not menu_button:
            try:
                menu_button = page.ele("css:img[src*='menu_ham.svg']", timeout=5)
                if menu_button:
                    logging.info("âœ… ë©”ë‰´ ë²„íŠ¼ ë°œê²¬ (ë°©ë²• 2: src ì†ì„±)")
            except:
                pass
        
        # ë°©ë²• 3: srcset ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
        if not menu_button:
            try:
                menu_button = page.ele("css:img[srcset*='menu_ham.svg']", timeout=5)
                if menu_button:
                    logging.info("âœ… ë©”ë‰´ ë²„íŠ¼ ë°œê²¬ (ë°©ë²• 3: srcset ì†ì„±)")
            except:
                pass
        
        # ë°©ë²• 4: XPathë¡œ ì°¾ê¸°
        if not menu_button:
            try:
                menu_button = page.ele("xpath://img[contains(@src, 'menu_ham.svg') or contains(@srcset, 'menu_ham.svg') or @alt='ë©”ë‰´ ë²„íŠ¼']", timeout=5)
                if menu_button:
                    logging.info("âœ… ë©”ë‰´ ë²„íŠ¼ ë°œê²¬ (ë°©ë²• 4: XPath)")
            except:
                pass
        
        if not menu_button:
            logging.warning("âš ï¸ ë©”ë‰´ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        
        # ë©”ë‰´ ë²„íŠ¼ì´ í´ë¦­ ê°€ëŠ¥í•œ ë¶€ëª¨ ìš”ì†Œ ì°¾ê¸° (ë²„íŠ¼ì´ë‚˜ ë§í¬ì¼ ìˆ˜ ìˆìŒ)
        clickable_element = menu_button
        try:
            # ë¶€ëª¨ ìš”ì†Œê°€ buttonì´ë‚˜ a íƒœê·¸ì¸ì§€ í™•ì¸
            parent = menu_button.parent()
            if parent and parent.tag in ['button', 'a']:
                clickable_element = parent
                logging.info(f"âœ… í´ë¦­ ê°€ëŠ¥í•œ ë¶€ëª¨ ìš”ì†Œ ë°œê²¬: <{parent.tag}>")
        except:
            pass
        
        # ë©”ë‰´ ë²„íŠ¼ í´ë¦­ (JS ê°•ì œ í´ë¦­)
        try:
            # ë°©ë²• 1: ì¼ë°˜ í´ë¦­ ì‹œë„
            try:
                # í™”ë©´ ì¤‘ì•™ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                page.run_js(
                    "arguments[0].scrollIntoView({block:'center', inline:'center'});",
                    clickable_element
                )
                time.sleep(0.3)
                clickable_element.click()
                logging.info("ğŸ”¥ [1ë‹¨ê³„ ì„±ê³µ] ë©”ë‰´ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ (ì¼ë°˜ í´ë¦­)")
            except Exception as e1:
                # ë°©ë²• 2: JS í´ë¦­ ì‹œë„
                logging.warning(f"âš ï¸ ì¼ë°˜ í´ë¦­ ì‹¤íŒ¨: {str(e1)[:100]}, JS í´ë¦­ ì‹œë„...")
                page.run_js("arguments[0].click();", clickable_element)
                logging.info("ğŸ”¥ [1ë‹¨ê³„ ì„±ê³µ] ë©”ë‰´ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ (JS í´ë¦­)")
        except Exception as e2:
            logging.error(f"âŒ ë©”ë‰´ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {str(e2)[:100]}")
            return False
        
        # ë©”ë‰´ê°€ ì—´ë¦¬ê¸¸ ê¸°ë‹¤ë¦¼
        time.sleep(2)
        random_delay(1.0, 2.0)
        
        # 2ë‹¨ê³„: /live-more ë§í¬ ì°¾ê¸° ë° í´ë¦­
        logging.info("ğŸ” [2ë‹¨ê³„] /live-more ë§í¬ ì°¾ê¸° ì‹œì‘...")
        
        live_more_link = None
        end_time = time.time() + timeout
        
        while time.time() < end_time:
            try:
                # ë°©ë²• 1: href ì†ì„±ìœ¼ë¡œ ì •í™•íˆ ì°¾ê¸°
                live_more_link = page.ele("css:a[href='/live-more']", timeout=2)
                if live_more_link:
                    logging.info("âœ… /live-more ë§í¬ ë°œê²¬ (ë°©ë²• 1)")
                    break
            except:
                pass
            
            try:
                # ë°©ë²• 2: hrefì— live-moreê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
                live_more_link = page.ele("css:a[href*='live-more']", timeout=2)
                if live_more_link:
                    logging.info("âœ… /live-more ë§í¬ ë°œê²¬ (ë°©ë²• 2)")
                    break
            except:
                pass
            
            try:
                # ë°©ë²• 3: XPathë¡œ ì°¾ê¸°
                live_more_link = page.ele("xpath://a[contains(@href, 'live-more')]", timeout=2)
                if live_more_link:
                    logging.info("âœ… /live-more ë§í¬ ë°œê²¬ (ë°©ë²• 3)")
                    break
            except:
                pass
            
            time.sleep(0.5)
        
        if not live_more_link:
            logging.warning("âš ï¸ /live-more ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        
        # /live-more ë§í¬ í´ë¦­ (JS ê°•ì œ í´ë¦­)
        try:
            # ë°©ë²• 1: ì¼ë°˜ í´ë¦­ ì‹œë„
            try:
                # í™”ë©´ ì¤‘ì•™ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                page.run_js(
                    "arguments[0].scrollIntoView({block:'center', inline:'center'});",
                    live_more_link
                )
                time.sleep(0.3)
                live_more_link.click()
                logging.info("ğŸ”¥ [2ë‹¨ê³„ ì„±ê³µ] /live-more ë§í¬ í´ë¦­ ì™„ë£Œ (ì¼ë°˜ í´ë¦­)")
            except Exception as e1:
                # ë°©ë²• 2: JS í´ë¦­ ì‹œë„
                logging.warning(f"âš ï¸ ì¼ë°˜ í´ë¦­ ì‹¤íŒ¨: {str(e1)[:100]}, JS í´ë¦­ ì‹œë„...")
                page.run_js("arguments[0].click();", live_more_link)
                logging.info("ğŸ”¥ [2ë‹¨ê³„ ì„±ê³µ] /live-more ë§í¬ í´ë¦­ ì™„ë£Œ (JS í´ë¦­)")
        except Exception as e2:
            logging.error(f"âŒ /live-more ë§í¬ í´ë¦­ ì‹¤íŒ¨: {str(e2)[:100]}")
            return False
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        
        logging.info("âœ… [ì „ì²´ ì„±ê³µ] ë©”ë‰´ ë²„íŠ¼ â†’ /live-more í´ë¦­ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logging.error(f"âŒ [ë™ì‘ ì‹¤íŒ¨] wait_and_mouse_click_live_more: {str(e)[:200]}")
        import traceback
        logging.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return False
    
BAD = "connectivitycheck.gstatic.com"

def get_with_newtab_check(page, url, page_timeout, watch_sec=2.0):
    # í˜„ì¬ íƒ­(ì›ë˜ íƒ­) ê°ì²´ + íƒ­ ëª©ë¡ ìŠ¤ëƒ…ìƒ·
    main_tab = page.get_tab()                 # Pageê°€ ì»¨íŠ¸ë¡¤ ì¤‘ì¸ íƒ­ :contentReference[oaicite:2]{index=2}
    before = set(page.tab_ids)                # ì „ì²´ íƒ­ id ë¦¬ìŠ¤íŠ¸ :contentReference[oaicite:3]{index=3}

    page.get(url, timeout=page_timeout)

    page.wait.ele_displayed('tag:body', timeout=page_timeout)
    
    # get() ì´í›„ ì ê¹ ê°ì‹œ: ìƒˆ íƒ­ì´ ëœ¨ëŠ”ì§€ í™•ì¸
    end = time.time() + watch_sec
    while time.time() < end:
        now = set(page.tab_ids)
        new_ids = list(now - before)
        if new_ids:
            for tid in new_ids:
                tab = page.get_tab(tid)       # ìƒˆ íƒ­ ê°ì²´ ì–»ê¸° :contentReference[oaicite:4]{index=4}
                tab_url = getattr(tab, "url", "") or ""
                if BAD in tab_url:
                    # ì›ì¹˜ ì•ŠëŠ” íƒ­ì´ë©´ ë‹«ê³  :contentReference[oaicite:5]{index=5}
                    page.close_tabs(tid)
                    # ì›ë˜ íƒ­ì„ ë‹¤ì‹œ ì•ìœ¼ë¡œ :contentReference[oaicite:6]{index=6}
                    main_tab.set.activate()
                    return True  # "connectivitycheck íƒ­ì´ ë–´ë‹¤"
            break
        time.sleep(0.05)

    return False  # ê·¸ëŸ° íƒ­ ì•ˆ ëœ¸    
# =============================================================================
# 7) ì‘ì—… ë¡œì§
# =============================================================================
def thread_worker(task: Dict, proxy: ProxyInfo, slot_id: str, r: redis.Redis):
    keyword, target_url = task["keyword"], task["domain"]
    logging.info(f"â–¶ï¸ ì‘ì—… ì‹œì‘ | ìŠ¬ë¡¯: {slot_id} | í‚¤ì›Œë“œ: [{keyword}] | í”„ë¡ì‹œ: {proxy.address}")

    browser_wrapper, page = None, None
    rr = RunResult(
        datetime.now().isoformat(timespec="seconds"),
        keyword, target_url,
        proxy.protocol, proxy.address, proxy.source,
        False, None, None, None,
        False, None, None, None
    )

    try:
        # 1. TCP ì²´í¬ ë° ë‚´ IP ìœ ì¶œ ê²€ì‚¬
        #if not tcp_quick_check(proxy.address):
        #    logging.warning(f"âŒ TCP ì—°ê²° ì‹¤íŒ¨: {proxy.address}")
        #    rr.error = "TCP_CONNECT_FAIL"

        if is_proxy_leaking_my_ip(proxy, MY_PUBLIC_IP):
            logging.warning(f"âŒ í”„ë¡ì‹œ ê±°ë¶€ (ë‚´ ê³µì¸ IP ë…¸ì¶œë¨): {proxy.address}")
            rr.error = "IP_LEAK_DETECTED"

        else:
            logging.info(f"ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘ (ìŠ¬ë¡¯ {slot_id})")
            browser_wrapper, page = make_stealth_browser(proxy, slot_id)

            random_delay(1.0, 2.0)
            logging.info(f"ğŸ” ë„¤ì´ë²„ ì ‘ì† ë° í‚¤ì›Œë“œ ê²€ìƒ‰: [{keyword}]")
            
            # ë„¤ì´ë²„ ì ‘ì†
            #page.get("https://www.naver.com/", timeout=PAGELOAD_TIMEOUT_SEC)
            if get_with_newtab_check(page, "https://m.naver.com/", PAGELOAD_TIMEOUT_SEC, watch_sec=2.0) :
                raise Exception("PROXY ERROR_CONNECTIVITYCHECK")

            time.sleep(2)

            random_delay(1.5, 3.0)
            simulate_scroll(page, scroll_count=2)
            #page.actions.click('#MM_SEARCH_FAKE').click('#query').type('í…ŒìŠ¤íŠ¸').key_down(Keys.ENTER).key_up(Keys.ENTER)
            ##########################################################
            page.actions.click('#MM_SEARCH_FAKE').click('#query')

            text = keyword
            for ch in text:
                page.actions.type(ch)
                time.sleep(random.uniform(0.5, 0.9))  # ê¸€ì ì‚¬ì´ ë”œë ˆì´(ì›í•˜ëŠ”ëŒ€ë¡œ)

            # ì—”í„°ë„ ì‚¬ëŒì²˜ëŸ¼ ì•½ê°„ ì‰¬ì—ˆë‹¤ê°€
            time.sleep(random.uniform(0.12, 0.35))
            page.actions.key_down(Keys.ENTER).key_up(Keys.ENTER)
            
            ##########################################################
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ëŒ€ê¸°
            page.wait.ele_displayed('tag:body', timeout=30)
            #time.sleep(3) 
            current_url = page.url
            if "search.naver.com" not in current_url:
                s = f"ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™í•˜ì§€ ëª»í•¨: {current_url}"
                raise Exception(s)
            
            results_url = current_url
            random_delay(2.0, 4.0)

            # í˜ì´ì§€ íƒìƒ‰
            for page_num in range(1, MAX_PAGES + 1):
                if STOP_EVENT.is_set():
                    break

                logging.info(f"ğŸ“„ í˜ì´ì§€ íƒìƒ‰ ì¤‘ ({page_num}/{MAX_PAGES} page)")
                page.get(update_query_param(results_url, start=1 + (page_num - 1) * 10), 
                        timeout=PAGELOAD_TIMEOUT_SEC)
                random_delay(2.0, 3.5)
                simulate_scroll(page, scroll_count=3)

                # ë§í¬ ì°¾ê¸°
                found_data = None
                anchors = page.eles("css:a[href]")

                t_can = urlunparse((
                    urlparse(target_url).scheme,
                    urlparse(target_url).netloc,
                    urlparse(target_url).path or "/",
                    "", "", ""
                )) if target_url else None

                for idx, a in enumerate(anchors, 1):
                    try:
                        href = a.attr("href") or ""
                        if href and target_url:
                            h_can = urlunparse((
                                urlparse(href).scheme,
                                urlparse(href).netloc,
                                urlparse(href).path or "/",
                                "", "", ""
                            ))

                            if h_can.lower() == t_can.lower():
                                found_data = (idx, href, a)
                                break
                    except:
                        continue

                if found_data:
                    rank, href, elem = found_data
                    rr.found = True
                    rr.found_page = page_num
                    rr.found_rank_on_page = rank
                    rr.found_href = href
                    
                    random_delay(1.0, 2.5)

                    # í´ë¦­
                    logging.info(f"[Slot-{slot_id}] ğŸ”— íƒ€ê²Ÿ ë§í¬ ë°œê²¬ (í˜ì´ì§€ {page_num}, ìˆœìœ„ {rank})")
                    
                    # ìŠ¤í¬ë¡¤í•˜ì—¬ ìš”ì†Œ ë³´ì´ê²Œ
                    try:
                        page.run_js(
                            "arguments[0].scrollIntoView({block:'center', inline:'center'});",
                            elem
                        )
                    except:
                        pass
                    random_delay(0.3, 0.8)

                    # í´ë¦­
                    elem.click()
                    logging.info(f"[Slot-{slot_id}] âœ… elem.click() executed")
                    time.sleep(3)

                    # ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤
                    random_delay(30.0, 60.0)
                    
                    # live-more í´ë¦­ ì‹œë„
                    wait_and_mouse_click_live_more(page)
                    
                    random_delay(30.0, 60.0)
                    simulate_natural_scroll(page)
                    random_delay(30.0, 36.0)

                    # ìµœì¢… URL í™•ì¸
                    final_url = page.url
                    h_final = urlunparse((
                        urlparse(final_url).scheme,
                        urlparse(final_url).netloc,
                        urlparse(final_url).path or "/",
                        "", "", ""
                    ))

                    if t_can and h_final.lower() == t_can.lower():
                        rr.clicked_ok = True
                        rr.final_url = final_url
                    else:
                        rr.clicked_ok = False
                        rr.final_url = final_url
                        rr.note = "FINAL_URL_NOT_MATCH"

                    break

                if page_num < MAX_PAGES:
                    random_delay(1.5, 3.0)

            if not rr.found and not rr.error:
                rr.error = "NOT_FOUND_IN_PAGES"

    except Exception as e:
        logging.error(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {str(e)[:100]}")
        rr.error = str(e)[:160]

    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        if browser_wrapper:
            try:
                browser_wrapper.quit()
            except:
                pass

        # í”„ë¡ì‹œ ë°˜ë‚©
        return_proxy_to_redis(r, proxy)

        # ê²°ê³¼ ì €ì¥
        with FILE_LOCK:
            with open(RESULT_JSONL, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(rr), ensure_ascii=False) + "\n")
            
            is_new = not os.path.exists(RESULT_CSV)
            with open(RESULT_CSV, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                if is_new:
                    w.writerow([
                        "ts", "keyword", "target_url", "proxy_protocol", "proxy_address", "proxy_source",
                        "found", "found_page", "found_rank_on_page", "found_href",
                        "clicked_ok", "final_url", "error", "note"
                    ])
                w.writerow([
                    rr.ts, rr.keyword, rr.target_url, rr.proxy_protocol, rr.proxy_address, rr.proxy_source,
                    rr.found, rr.found_page, rr.found_rank_on_page, rr.found_href,
                    rr.clicked_ok, rr.final_url, rr.error, rr.note
                ])

        logging.info(f"ğŸ ì‘ì—… ì¢…ë£Œ | ìŠ¬ë¡¯: {slot_id} | ê²°ê³¼: {'ì„±ê³µ' if rr.found else 'ì‹¤íŒ¨'}")

# =============================================================================
# 8) ë©”ì¸ ë£¨í”„
# =============================================================================
def main_loop() -> None:
    global MY_PUBLIC_IP
    setup_logging()
    logging.info("==================================================")
    logging.info("ğŸš€ Naver Exposure Monitor ì‹œì‘")
    
    # ë‚´ ê³µì¸ IP í™•ì¸
    MY_PUBLIC_IP = get_my_actual_ip()
    logging.info(f"ğŸ  ë‚´ ê³µì¸ IP: {MY_PUBLIC_IP}")
    
    logging.info(f"âš™ï¸ ì„¤ì •: ìŠ¤ë ˆë“œ ìŠ¬ë¡¯ {MAX_THREADS}ê°œ / íƒìƒ‰ {MAX_PAGES}í˜ì´ì§€")
    logging.info("==================================================")
    
    # Redis ì—°ê²°
    try:
        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
        r.ping()
        logging.info("âœ… Redis ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logging.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    proxies_cache = []
    active_threads: List[threading.Thread] = []
    
    try:
        while not STOP_EVENT.is_set():
            # í”„ë¡ì‹œ ìˆ˜ì§‘ (Redisì—ì„œ)
            if REFRESH_PROXIES_EACH_CYCLE or not proxies_cache:
                proxies_cache = fetch_proxies_from_redis(r, max_count=MAX_PROXIES_PER_TASK * len(TASKS))
            
            if not proxies_cache:
                logging.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. 60ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                time.sleep(60)
                continue

            # ê° íƒœìŠ¤í¬ì— ëŒ€í•´ í”„ë¡ì‹œ í• ë‹¹
            for task in TASKS:
                for idx, proxy in enumerate(proxies_cache[:MAX_PROXIES_PER_TASK]):
                    if STOP_EVENT.is_set():
                        break
                    
                    # í™œì„± ìŠ¤ë ˆë“œ ì •ë¦¬
                    active_threads = [t for t in active_threads if t.is_alive()]
                    
                    # ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ ëŒ€ê¸°
                    while len(active_threads) >= MAX_THREADS:
                        active_threads = [t for t in active_threads if t.is_alive()]
                        time.sleep(1)
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¬ë¡¯ ì°¾ê¸°
                    used_slots = set()
                    for t in active_threads:
                        if t.is_alive() and '-slot' in t.name:
                            try:
                                used_slots.add(int(t.name.split('-slot')[-1]))
                            except:
                                pass
                    
                    available_slot = None
                    for slot_num in range(MAX_THREADS):
                        if slot_num not in used_slots:
                            available_slot = slot_num
                            break
                    if available_slot is None:
                        available_slot = 0
                    
                    slot_id = str(available_slot)
                    
                    # Redisì—ì„œ í”„ë¡ì‹œ ì„ëŒ€
                    proxy_str = f"{proxy.protocol}://{proxy.address}"
                    claimed = r.eval(_LUA_CLAIM, 2, REDIS_ZSET_ALIVE, REDIS_ZSET_LEASE, 
                                   int(time.time()), 600)
                    
                    if not claimed:
                        logging.warning(f"âš ï¸ í”„ë¡ì‹œ ì„ëŒ€ ì‹¤íŒ¨: {proxy_str}")
                        continue
                    
                    # ìŠ¤ë ˆë“œ ì‹œì‘
                    t_name = f"{task['keyword']}-{idx}-slot{slot_id}"
                    t = threading.Thread(
                        target=thread_worker, 
                        args=(task, proxy, slot_id, r), 
                        name=t_name, 
                        daemon=True
                    )
                    active_threads.append(t)
                    t.start()
                    logging.info(f"â• ìƒˆ ìŠ¤ë ˆë“œ í• ë‹¹: [{t_name}]")
                    time.sleep(2)  # ìˆœì°¨ì  ìƒì„±

            # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
            while any(t.is_alive() for t in active_threads):
                active_threads = [t for t in active_threads if t.is_alive()]
                time.sleep(2)
            
            logging.info(f"âœ… ì‚¬ì´í´ ì™„ë£Œ. {CHECK_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°...")
            time.sleep(CHECK_INTERVAL_SECONDS)
            
    except KeyboardInterrupt:
        STOP_EVENT.set()
        logging.info("ğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    finally:
        # ëª¨ë“  í™œì„± ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        for t in active_threads:
            if t.is_alive():
                t.join(timeout=10)

if __name__ == "__main__":
    main_loop()